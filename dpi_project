{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11707627,"sourceType":"datasetVersion","datasetId":7348383}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rdkit-pypi rdkit torch transformers pytorch-lightning requests matplotlib -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport gc\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nfrom rdkit import Chem, RDLogger, DataStructs\nfrom rdkit.Chem import AllChem\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom scipy.stats import pearsonr, spearmanr\nfrom typing import Dict, Optional, List, Any\nimport pandas as pd\nimport numpy as np\nimport logging\nimport psutil\n\n# Suppress RDKit warnings\nRDLogger.DisableLog('rdApp.*')\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n    handlers=[logging.StreamHandler(sys.stdout)],\n    force=True\n)\nlogger = logging.getLogger(__name__)\nlogger.info(\"Logging initialized\")\n\ndef seed_everything(seed: int = 42) -> None:\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\nseed_everything()\n\ndef log_memory_usage() -> None:\n    process = psutil.Process()\n    mem_info = process.memory_info()\n    logger.info(f\"Memory usage: {mem_info.rss / 1024**2:.2f} MB\")\n\ndef canonicalize_smiles(smiles: str) -> Optional[str]:\n    try:\n        mol = Chem.MolFromSmiles(smiles)\n        if mol is None:\n            return None\n        return Chem.MolToSmiles(mol, canonical=True)\n    except Exception as e:\n        logger.error(f\"SMILES canonicalization failed: {e}\")\n        return None\n\ndef load_bindingdb(path: str, max_samples: int = 20000, chunksize: int = 100000) -> Optional[pd.DataFrame]:\n    logger.info(f\"Loading BindingDB from {path}\")\n    try:\n        required_cols = [\"Ligand SMILES\", \"Kd (nM)\", \"BindingDB Target Chain Sequence\"]\n        df_chunk = pd.read_csv(path, sep=\"\\t\", low_memory=False, nrows=10)\n        if not all(col in df_chunk.columns for col in required_cols):\n            logger.error(f\"Missing columns: {set(required_cols) - set(df_chunk.columns)}\")\n            return None\n        chunks = []\n        total_rows = 0\n        invalid_smiles = 0\n        for chunk in pd.read_csv(path, sep=\"\\t\", usecols=required_cols, chunksize=chunksize):\n            total_rows += len(chunk)\n            chunk = chunk.dropna()\n            chunk[\"SMILES\"] = chunk[\"Ligand SMILES\"].apply(canonicalize_smiles)\n            invalid_smiles += len(chunk) - chunk[\"SMILES\"].notna().sum()\n            chunk = chunk[chunk[\"SMILES\"].notna()]\n            chunk[\"Kd (nM)\"] = pd.to_numeric(chunk[\"Kd (nM)\"], errors=\"coerce\")\n            chunk = chunk.dropna(subset=[\"Kd (nM)\"])\n            chunk = chunk[chunk[\"Kd (nM)\"].between(0.1, 1e6)]\n            chunks.append(chunk)\n            if len(pd.concat(chunks)) >= max_samples:\n                break\n        if not chunks:\n            logger.error(\"No valid BindingDB data\")\n            return None\n        df = pd.concat(chunks).head(max_samples)\n        df[\"log_kd\"] = -np.log10(df[\"Kd (nM)\"] / 1e9)\n        df = df[df[\"log_kd\"].between(-10, 10)]\n        df = df.rename(columns={\"BindingDB Target Chain Sequence\": \"Target Sequence\"})\n        df = df.drop_duplicates(subset=[\"SMILES\"])\n        logger.info(f\"Loaded BindingDB dataset with {len(df)} samples\")\n        logger.info(f\"NaNs in log_kd: {df['log_kd'].isna().sum()}\")\n        logger.info(f\"Total rows processed: {total_rows}, Invalid SMILES: {invalid_smiles} ({invalid_smiles/total_rows*100:.2f}%)\")\n        logger.info(f\"BindingDB log_kd mean: {df['log_kd'].mean():.2f}, std: {df['log_kd'].std():.2f}\")\n        log_memory_usage()\n        return df[[\"SMILES\", \"Target Sequence\", \"log_kd\"]]\n    except Exception as e:\n        logger.error(f\"Error loading BindingDB: {e}\")\n        return None\n\nclass CustomSMILESTokenizer:\n    def __init__(self, max_len: int = 128):\n        self.vocab = list(\"CcNnOoSsClBr#=()[]-+1234567890@%\\\\\")\n        self.token2id = {t: i + 1 for i, t in enumerate(self.vocab)}\n        self.max_len = max_len\n    def tokenize(self, smiles: str) -> Dict[str, torch.Tensor]:\n        tokens = [self.token2id.get(c, 0) for c in smiles if c in self.token2id]\n        tokens = tokens[:self.max_len]\n        tokens += [0] * (self.max_len - len(tokens))\n        attention_mask = [1 if t != 0 else 0 for t in tokens]\n        return {\n            \"input_ids\": torch.tensor(tokens, dtype=torch.long),\n            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long)\n        }\n\nclass SMILESProteinTokenizer:\n    def __init__(self, max_len_smiles: int = 128, max_len_protein: int = 512):\n        self.max_len_smiles = max_len_smiles\n        self.max_len_protein = max_len_protein\n        try:\n            logger.info(\"Loading Hugging Face tokenizers\")\n            self.smiles_tokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n            self.protein_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")\n            self.use_custom = False\n        except Exception as e:\n            logger.warning(f\"Failed to load SMILES tokenizer: {e}. Using custom.\")\n            self.smiles_tokenizer = CustomSMILESTokenizer(max_len_smiles)\n            self.protein_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")\n            self.use_custom = True\n    def tokenize(self, smiles: str, protein: str) -> Optional[Dict[str, torch.Tensor]]:\n        try:\n            if not isinstance(smiles, str) or not isinstance(protein, str):\n                logger.error(f\"Invalid input types: SMILES={type(smiles)}, Protein={type(protein)}\")\n                return None\n            if self.use_custom:\n                smiles_enc = self.smiles_tokenizer.tokenize(smiles)\n            else:\n                smiles_enc = self.smiles_tokenizer(\n                    smiles, max_length=self.max_len_smiles, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n                )\n            protein_enc = self.protein_tokenizer(\n                protein, max_length=self.max_len_protein, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n            )\n            return {\n                \"smiles_input_ids\": smiles_enc[\"input_ids\"].squeeze(),\n                \"smiles_attention_mask\": smiles_enc[\"attention_mask\"].squeeze(),\n                \"protein_input_ids\": protein_enc[\"input_ids\"].squeeze(),\n                \"protein_attention_mask\": protein_enc[\"attention_mask\"].squeeze()\n            }\n        except Exception as e:\n            logger.error(f\"Tokenization error: {e}\")\n            return None\n\nclass DTIDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, tokenizer: SMILESProteinTokenizer, log_kd_mean: float = 0.0, log_kd_std: float = 1.0, randomize_smiles_flag: bool = False):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.log_kd_mean = log_kd_mean\n        self.log_kd_std = log_kd_std\n        self.failed_samples: List[int] = []\n        self.randomize_smiles_flag = randomize_smiles_flag\n        logger.info(f\"Creating dataset with {len(df)} samples, randomize_smiles={randomize_smiles_flag}\")\n    def __len__(self) -> int:\n        return len(self.df)\n    def __getitem__(self, idx: int) -> Optional[Dict[str, Any]]:\n        row = self.df.iloc[idx]\n        try:\n            smiles, protein, log_kd = row[\"SMILES\"], row[\"Target Sequence\"], row[\"log_kd\"]\n            if self.randomize_smiles_flag and hasattr(self, 'training') and self.training:\n                randomized = randomize_smiles(smiles)\n                if randomized is not None:\n                    smiles = randomized\n            if not isinstance(smiles, str) or not isinstance(protein, str) or not isinstance(log_kd, (int, float)):\n                logger.warning(f\"Invalid data at idx {idx}: SMILES={type(smiles)}, Protein={type(protein)}, log_kd={type(log_kd)}\")\n                self.failed_samples.append(idx)\n                return None\n            if Chem.MolFromSmiles(smiles) is None:\n                logger.warning(f\"Invalid SMILES at idx {idx}\")\n                self.failed_samples.append(idx)\n                return None\n            tokens = self.tokenizer.tokenize(smiles, protein)\n            if tokens is None:\n                self.failed_samples.append(idx)\n                return None\n            normalized_log_kd = (log_kd - self.log_kd_mean) / self.log_kd_std\n            return {\n                **tokens,\n                \"log_kd\": torch.tensor(normalized_log_kd, dtype=torch.float32),\n                \"original_log_kd\": torch.tensor(log_kd, dtype=torch.float32),\n                \"smiles\": smiles,\n                \"protein\": protein\n            }\n        except Exception as e:\n            logger.error(f\"Dataset error at idx {idx}: {e}\")\n            self.failed_samples.append(idx)\n            return None\n\ndef collate_fn(batch: List[Optional[Dict]]) -> Optional[Dict[str, Any]]:\n    batch = [b for b in batch if b is not None]\n    if not batch:\n        logger.warning(\"Empty batch after filtering\")\n        return None\n    return {\n        \"smiles_input_ids\": torch.stack([b[\"smiles_input_ids\"] for b in batch]),\n        \"smiles_attention_mask\": torch.stack([b[\"smiles_attention_mask\"] for b in batch]),\n        \"protein_input_ids\": torch.stack([b[\"protein_input_ids\"] for b in batch]),\n        \"protein_attention_mask\": torch.stack([b[\"protein_attention_mask\"] for b in batch]),\n        \"log_kd\": torch.stack([b[\"log_kd\"] for b in batch]),\n        \"original_log_kd\": torch.stack([b[\"original_log_kd\"] for b in batch]),\n        \"smiles\": [b[\"smiles\"] for b in batch],\n        \"protein\": [b[\"protein\"] for b in batch]\n    }\n\ndef create_bindingdb_loader(bindingdb_path: str, tokenizer: SMILESProteinTokenizer, batch_size: int = 16, max_samples: int = 20000) -> tuple:\n    logger.info(\"Creating BindingDB DataLoaders\")\n    bindingdb_df = load_bindingdb(bindingdb_path, max_samples=max_samples)\n    if bindingdb_df is None or len(bindingdb_df) == 0:\n        logger.warning(\"Empty BindingDB dataset\")\n        bindingdb_df = pd.DataFrame(columns=[\"SMILES\", \"Target Sequence\", \"log_kd\"])\n    train_bindingdb = bindingdb_df.sample(frac=0.8, random_state=42)\n    val_bindingdb = bindingdb_df.drop(train_bindingdb.index).sample(frac=0.5, random_state=42)\n    test_bindingdb = bindingdb_df.drop(train_bindingdb.index).drop(val_bindingdb.index)\n    log_kd_mean = train_bindingdb[\"log_kd\"].mean()\n    log_kd_std = train_bindingdb[\"log_kd\"].std()\n    if log_kd_std == 0:\n        log_kd_std = 1.0\n    logger.info(f\"Normalization parameters - log_kd_mean: {log_kd_mean:.2f}, log_kd_std: {log_kd_std:.2f}\")\n    logger.info(f\"BindingDB splits - Train: {len(train_bindingdb)}, Val: {len(val_bindingdb)}, Test: {len(test_bindingdb)}\")\n    train_dataset = DTIDataset(train_bindingdb, tokenizer, log_kd_mean, log_kd_std, randomize_smiles_flag=True)\n    val_dataset = DTIDataset(val_bindingdb, tokenizer, log_kd_mean, log_kd_std, randomize_smiles_flag=False)\n    test_dataset = DTIDataset(test_bindingdb, tokenizer, log_kd_mean, log_kd_std, randomize_smiles_flag=False)\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=collate_fn) if len(train_dataset) > 0 else None\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=2, collate_fn=collate_fn) if len(val_dataset) > 0 else None\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=2, collate_fn=collate_fn) if len(test_dataset) > 0 else None\n    for dataset, name in [\n        (train_dataset, \"BindingDB Train\"),\n        (val_dataset, \"BindingDB Val\"),\n        (test_dataset, \"BindingDB Test\")\n    ]:\n        logger.info(f\"{name} failed samples: {len(dataset.failed_samples)} / {len(dataset)}\")\n    log_memory_usage()\n    return train_loader, val_loader, test_loader, log_kd_mean, log_kd_std\n\nclass CrossAttention(nn.Module):\n    def __init__(self, dim: int, num_heads: int = 8):\n        super().__init__()\n        self.multihead_attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, key_padding_mask: torch.Tensor = None) -> torch.Tensor:\n        if key_padding_mask is not None:\n            key_padding_mask = ~key_padding_mask.bool()\n        attn_output, _ = self.multihead_attn(query, key, value, key_padding_mask=key_padding_mask)\n        return attn_output\n\ndef correlation_loss(pred, target):\n    pred = pred.float()\n    target = target.float()\n    pred_mean = pred.mean()\n    target_mean = target.mean()\n    pred_centered = pred - pred_mean\n    target_centered = target - target_mean\n    numerator = (pred_centered * target_centered).sum()\n    denominator = torch.sqrt((pred_centered ** 2).sum() * (target_centered ** 2).sum()) + 1e-8\n    corr = numerator / denominator\n    return 1 - corr\n\nclass DTIModel(pl.LightningModule):\n    def __init__(self, smiles_model_name: str = \"seyonec/PubChem10M_SMILES_BPE_450k\", protein_model_name: str = \"facebook/esm2_t12_35M_UR50D\", hidden_dim: int = 256, learning_rate: float = 1e-5, log_kd_mean: float = 0.0, log_kd_std: float = 1.0, dropout: float = 0.2, combined_loss: bool = False, weight_decay: float = 0.01):\n        super().__init__()\n        self.save_hyperparameters()\n        self.smiles_encoder = AutoModel.from_pretrained(smiles_model_name)\n        self.protein_encoder = AutoModel.from_pretrained(protein_model_name)\n        self.smiles_dim = self.smiles_encoder.config.hidden_size\n        self.protein_dim = self.protein_encoder.config.hidden_size\n        self.protein_preproj = nn.Linear(self.protein_dim, self.smiles_dim)\n        self.cross_attention = CrossAttention(self.smiles_dim, num_heads=8)\n        self.smiles_proj = nn.Linear(self.smiles_dim, hidden_dim)\n        self.protein_proj = nn.Linear(self.smiles_dim, hidden_dim)\n        self.regression_head = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, 1)\n        )\n        self.criterion = nn.MSELoss()\n        self.combined_loss = combined_loss\n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        self.test_predictions = []\n        self.test_targets = []\n        self.test_smiles = []\n        self.test_proteins = []\n        self.dataset_name = \"BindingDB\"\n        self.is_finetuning = False\n        self.log_kd_mean = log_kd_mean\n        self.log_kd_std = log_kd_std\n        # For loss logging\n        self.train_losses = []\n        self.val_losses = []\n        self.train_corr_losses = []\n        self.epoch_indices = []\n    def forward(self, smiles_input_ids, smiles_attention_mask, protein_input_ids, protein_attention_mask):\n        with torch.amp.autocast('cuda', enabled=torch.cuda.is_available()):\n            smiles_embeds = self.smiles_encoder(input_ids=smiles_input_ids, attention_mask=smiles_attention_mask).last_hidden_state\n            protein_embeds = self.protein_encoder(input_ids=protein_input_ids, attention_mask=protein_attention_mask).last_hidden_state\n            protein_embeds = self.protein_preproj(protein_embeds)\n            combined_embeds = self.cross_attention(query=smiles_embeds, key=protein_embeds, value=protein_embeds, key_padding_mask=protein_attention_mask)\n            smiles_pooled = combined_embeds.mean(dim=1)\n            protein_pooled = protein_embeds.mean(dim=1)\n            smiles_pooled = self.smiles_proj(smiles_pooled)\n            protein_pooled = self.protein_proj(protein_pooled)\n            combined = torch.cat([smiles_pooled, protein_pooled], dim=-1)\n            log_kd_pred = self.regression_head(combined).squeeze(-1)\n            return log_kd_pred\n    def training_step(self, batch, batch_idx):\n        if batch is None:\n            logger.warning(\"None batch in training_step\")\n            return None\n        batch_size = batch[\"smiles_input_ids\"].shape[0]\n        assert batch_size > 0, \"Empty batch detected\"\n        log_kd_pred = self(batch[\"smiles_input_ids\"], batch[\"smiles_attention_mask\"], batch[\"protein_input_ids\"], batch[\"protein_attention_mask\"])\n        targets = batch[\"original_log_kd\"]\n        loss = self.criterion(log_kd_pred, targets)\n        corr_loss_val = correlation_loss(log_kd_pred, targets)\n        if self.combined_loss:\n            total_loss = loss + corr_loss_val\n        else:\n            total_loss = loss\n        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n        self.log(\"train_corr_loss\", corr_loss_val, on_step=False, on_epoch=True, batch_size=batch_size)\n        # Log for plotting\n        if self.trainer and self.trainer.logger is not None and self.trainer.global_rank == 0:\n            self.train_losses.append(loss.item())\n            self.train_corr_losses.append(corr_loss_val.item())\n        return total_loss\n    def validation_step(self, batch, batch_idx):\n        if batch is None:\n            logger.warning(\"None batch in validation_step\")\n            return None\n        batch_size = batch[\"smiles_input_ids\"].shape[0]\n        assert batch_size > 0, \"Empty batch in validation_step\"\n        log_kd_pred = self(batch[\"smiles_input_ids\"], batch[\"smiles_attention_mask\"], batch[\"protein_input_ids\"], batch[\"protein_attention_mask\"])\n        targets = batch[\"original_log_kd\"]\n        loss = self.criterion(log_kd_pred, targets)\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n        # Log for plotting\n        if self.trainer and self.trainer.logger is not None and self.trainer.global_rank == 0:\n            self.val_losses.append(loss.item())\n        return loss\n    def on_train_epoch_end(self):\n        # Log epoch index for plotting\n        if self.trainer and self.trainer.logger is not None and self.trainer.global_rank == 0:\n            self.epoch_indices.append(self.current_epoch)\n    def test_step(self, batch, batch_idx):\n        if batch is None:\n            logger.warning(\"None batch in test_step\")\n            return None\n        batch_size = batch[\"smiles_input_ids\"].shape[0]\n        assert batch_size > 0, \"Empty batch in test_step\"\n        log_kd_pred = self(batch[\"smiles_input_ids\"], batch[\"smiles_attention_mask\"], batch[\"protein_input_ids\"], batch[\"protein_attention_mask\"])\n        targets = batch[\"original_log_kd\"]\n        loss = self.criterion(log_kd_pred, targets)\n        self.test_predictions.append(log_kd_pred.cpu())\n        self.test_targets.append(targets.cpu())\n        self.test_smiles.extend(batch[\"smiles\"])\n        self.test_proteins.extend(batch[\"protein\"])\n        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, batch_size=batch_size)\n        return loss\n    def on_test_epoch_end(self):\n        predictions = torch.cat(self.test_predictions).numpy()\n        targets = torch.cat(self.test_targets).numpy()\n        test_loss = self.criterion(torch.tensor(predictions), torch.tensor(targets)).item()\n        test_mae = mean_absolute_error(targets, predictions)\n        test_rmse = np.sqrt(mean_squared_error(targets, predictions))\n        test_r2 = r2_score(targets, predictions) if len(set(targets)) > 1 else np.nan\n        test_pearson = pearsonr(targets, predictions)[0] if np.var(targets) > 1e-6 and np.var(predictions) > 1e-6 else np.nan\n        logger.info(f\"Test predictions mean: {predictions.mean():.2f}, std: {predictions.std():.2f}\")\n        logger.info(f\"Test targets mean: {targets.mean():.2f}, std: {targets.std():.2f}\")\n        logger.info(f\"Test metrics: loss={test_loss:.3f}, mae={test_mae:.3f}, rmse={test_rmse:.3f}, r2={test_r2}, pearson={test_pearson}\")\n        pd.DataFrame({\n            \"predictions\": predictions,\n            \"targets\": targets,\n            \"smiles\": self.test_smiles,\n            \"proteins\": self.test_proteins\n        }).to_csv(f\"test_predictions_{self.dataset_name}.csv\", index=False)\n        self.test_predictions = []\n        self.test_targets = []\n        self.test_smiles = []\n        self.test_proteins = []\n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4, verbose=True)\n        return {\n            \"optimizer\": optimizer,\n            \"lr_scheduler\": {\n                \"scheduler\": scheduler,\n                \"monitor\": \"val_loss\",\n                \"interval\": \"epoch\",\n                \"frequency\": 1\n            },\n            \"monitor\": \"val_loss\",\n            \"gradient_clip_val\": 1.0\n        }\n\ndef randomize_smiles(smiles: str, random_type: str = \"restricted\") -> Optional[str]:\n    try:\n        mol = Chem.MolFromSmiles(smiles)\n        if mol is None:\n            return None\n        if random_type == \"unrestricted\":\n            return Chem.MolToSmiles(mol, doRandom=True)\n        else:\n            return Chem.MolToSmiles(mol, doRandom=True, canonical=False)\n    except Exception as e:\n        logger.error(f\"SMILES randomization failed: {e}\")\n        return None\n\n# Set hyperparameters here for notebook use\nlearning_rate = 1e-5\nbatch_size = 16\nhidden_dim = 384  # Increased hidden_dim for more model capacity\ndropout = 0.3     # Increased dropout for regularization\nrandomize_smiles = True  # Enable SMILES randomization for data augmentation\ncombined_loss = True     # Use combined MSE + correlation loss\nmax_samples = 30000      # Use more data if possible\n# Reduce epochs to fit Kaggle runtime (e.g., 30-40)\nepochs = 25              # Lowered from 120 to 25 to avoid runtime cutoff\nweight_decay = 0.05      # Stronger regularization\nensemble_size = 3        # Number of models to ensemble at test time\nbindingdb_path = \"/kaggle/input/bindingdb/BindingDB_All.tsv\"\ncheckpoint_dir = \"/kaggle/working/checkpoints\"\nlog_dir = \"/kaggle/working/logs\"\nloss_log_path = \"output/loss_log.csv\"\nloss_plot_path = \"output/loss_plot.png\"\n\n# Create output directory if not exists\nos.makedirs(\"output\", exist_ok=True)\n\ntry:\n    import matplotlib.pyplot as plt\n    MATPLOTLIB_AVAILABLE = True\nexcept ImportError:\n    logger.warning(\"matplotlib is not installed. Loss plots will not be generated.\")\n    MATPLOTLIB_AVAILABLE = False\n\nif __name__ == \"__main__\":\n    try:\n        logger.info(\"[Kaggle Runtime Tip] Training for too many epochs may exceed the 12-hour limit. Current max_epochs is set to 35.\")\n        if not os.path.exists(bindingdb_path):\n            logger.error(f\"BindingDB path missing: {bindingdb_path}\")\n            raise FileNotFoundError(f\"BindingDB path missing: {bindingdb_path}\")\n        dataloaders = create_bindingdb_loader(bindingdb_path, SMILESProteinTokenizer(), batch_size=batch_size, max_samples=max_samples)\n        if dataloaders is None:\n            logger.error(\"Failed to create dataloaders\")\n            raise RuntimeError(\"Failed to create dataloaders\")\n        train_loader, val_loader, test_loader, log_kd_mean, log_kd_std = dataloaders\n        models = []\n        for ens_idx in range(ensemble_size):\n            logger.info(f\"Training ensemble model {ens_idx+1}/{ensemble_size}\")\n            model = DTIModel(\n                hidden_dim=hidden_dim,\n                learning_rate=learning_rate,\n                log_kd_mean=log_kd_mean,\n                log_kd_std=log_kd_std,\n                dropout=dropout,\n                combined_loss=combined_loss,\n                weight_decay=weight_decay\n            )\n            logger.info(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")\n            log_memory_usage()\n            checkpoint_callback = ModelCheckpoint(\n                dirpath=checkpoint_dir,\n                filename=f\"dti-model-ens{ens_idx}-{{epoch:02d}}-{{val_loss:.2f}}\",\n                monitor=\"val_loss\",\n                mode=\"min\",\n                save_top_k=2,  # Save more checkpoints in case of runtime cutoff\n                every_n_epochs=1  # Save every epoch\n            )\n            early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")  # More aggressive early stopping\n            callbacks = [checkpoint_callback, early_stopping]\n            trainer = pl.Trainer(\n                max_epochs=epochs,\n                accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n                devices=1,\n                callbacks=callbacks,\n                logger=pl.loggers.TensorBoardLogger(log_dir, name=f\"dti_ens{ens_idx}\"),\n                precision=\"16-mixed\",\n                enable_progress_bar=True\n            )\n            logger.info(\"Starting BindingDB training\")\n            if train_loader is not None and val_loader is not None:\n                trainer.fit(model, train_loader, val_loader)\n                logger.info(f\"Best checkpoint: {checkpoint_callback.best_model_path}\")\n                log_memory_usage()\n                torch.cuda.empty_cache()\n            checkpoint_path = checkpoint_callback.best_model_path\n            if checkpoint_path and os.path.isfile(checkpoint_path) and os.path.getsize(checkpoint_path) > 0:\n                logger.info(f\"Testing checkpoint: {checkpoint_path}\")\n                bindingdb_model = DTIModel.load_from_checkpoint(checkpoint_path, log_kd_mean=log_kd_mean, log_kd_std=log_kd_std)\n                bindingdb_model.dataset_name = \"BindingDB\"\n                if test_loader is not None:\n                    test_results = trainer.test(bindingdb_model, test_loader)\n                    logger.info(f\"BindingDB Test Results: {test_results}\")\n                models.append(bindingdb_model)\n            gc.collect()\n            torch.cuda.empty_cache()\n            log_memory_usage()\n        # Save and plot loss curves for the last model\n        try:\n            last_model = model\n            loss_df = pd.DataFrame({\n                \"epoch\": last_model.epoch_indices,\n                \"train_loss\": last_model.train_losses,\n                \"val_loss\": last_model.val_losses,\n                \"train_corr_loss\": last_model.train_corr_losses\n            })\n            loss_df.to_csv(loss_log_path, index=False)\n            if MATPLOTLIB_AVAILABLE:\n                plt.figure(figsize=(10,6))\n                plt.plot(loss_df[\"epoch\"], loss_df[\"train_loss\"], label=\"Train Loss\")\n                plt.plot(loss_df[\"epoch\"], loss_df[\"val_loss\"], label=\"Val Loss\")\n                plt.plot(loss_df[\"epoch\"], loss_df[\"train_corr_loss\"], label=\"Train Corr Loss\")\n                plt.xlabel(\"Epoch\")\n                plt.ylabel(\"Loss\")\n                plt.title(\"Loss Curves Over Epochs\")\n                plt.legend()\n                plt.tight_layout()\n                plt.savefig(loss_plot_path)\n                plt.close()\n                logger.info(f\"Saved loss log to {loss_log_path} and plot to {loss_plot_path}\")\n            else:\n                logger.warning(\"matplotlib not available, skipping loss plot.\")\n        except Exception as e:\n            logger.error(f\"Exception during loss plotting/saving: {e}\")\n        # Ensemble prediction (average)\n        try:\n            if len(models) > 1 and test_loader is not None:\n                logger.info(\"Running ensemble prediction on test set...\")\n                all_preds = []\n                for m in models:\n                    m.eval()\n                    preds = []\n                    with torch.no_grad():\n                        for batch in test_loader:\n                            if batch is None:\n                                continue\n                            pred = m(\n                                batch[\"smiles_input_ids\"].to(m.device),\n                                batch[\"smiles_attention_mask\"].to(m.device),\n                                batch[\"protein_input_ids\"].to(m.device),\n                                batch[\"protein_attention_mask\"].to(m.device)\n                            )\n                            preds.append(pred.cpu().numpy())\n                ensemble_preds = np.mean(np.stack(all_preds), axis=0)\n                # Save ensemble predictions\n                pd.DataFrame({\n                    \"ensemble_pred\": ensemble_preds\n                }).to_csv(\"output/ensemble_predictions.csv\", index=False)\n                logger.info(\"Saved ensemble predictions to output/ensemble_predictions.csv\")\n        except Exception as e:\n            logger.error(f\"Exception during ensemble prediction: {e}\")\n        gc.collect()\n        torch.cuda.empty_cache()\n        log_memory_usage()\n    except Exception as e:\n        logger.error(f\"Exception in main: {e}\")\n        gc.collect()\n        torch.cuda.empty_cache()\n        log_memory_usage()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}